{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the codes developed, openTURNS style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/simady/anaconda/envs/stochastic_field_env/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import openturns as ot\n",
    "import numpy as np\n",
    "import PureBeamExample as pbe #Toy model to analyse\n",
    "import KarhunenLoeveFieldSensitivity as klfs #Openturns integration of the analysis algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how the codes developed for the sensitivity anakysis on models governed by stochastic fields can be easily integrated in the actual openturns environment. \n",
    "\n",
    "We will use the toy example of the bending beam to demonstrate the method.\n",
    "\n",
    "##### The behaviour of the algorithms is highly inspired of the ones found in the openTURNS API\n",
    "\n",
    "###### The codes are composed of four parts that behave together :\n",
    "    ot.AggregatedKarhunenLoeveResults\n",
    "    ot.KarhunenLoeveSobolIndicesExperiment\n",
    "    ot.KarhunenLoeveGeneralizedFunctionWrapper\n",
    "    ot.SobolKarhunenLoeveFieldSensitivityAlgorithm\n",
    "    \n",
    "\n",
    "##### We are in the case where the model to analyse takes as an input mulitple stochastic fields and random variables. The model also returns multiple fields and scalar values. \n",
    "\n",
    "To be able to use random variables in our AggregatedKarhunenLoeveObject, we express our random variables as a constant stochastic field. In this way, we can still use our KarhunenLoeveDecomposition on it, but we will have to convert the constant field into a scalar value again just before passing it to our model. \n",
    "This must be coded manually. \n",
    "\n",
    "### Definition of the input processes and random variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a scalar distribution into a field : \n",
    "\n",
    "def variablesAsProcess(distribution, mesh):\n",
    "    '''Function to transform a scalar distribution into \n",
    "    a constant process defined over a mesh\n",
    "    '''\n",
    "    basis = ot.Basis([ot.SymbolicFunction(['x'],['1'])])\n",
    "    lawAsprocess = ot.FunctionalBasisProcess(distribution, basis, mesh)\n",
    "    lawAsprocess.setName(distribution.getName())\n",
    "    return lawAsprocess\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# First step : DEFINING THE PROCESSES AND RANDOM VARIABLES\n",
    "# First Process E_ : Youngs modulus\n",
    "# dimension \n",
    "dimension = 1\n",
    "#grid\n",
    "#Number of elements:\n",
    "NElem = [100]\n",
    "mesher = ot.IntervalMesher(NElem)\n",
    "lowBound = [0] #mm\n",
    "highBound = [1000] #mm\n",
    "interval = ot.Interval(lowBound,highBound)\n",
    "mesh = mesher.build(interval)\n",
    "\n",
    "#Covariance model Young's modulus\n",
    "amplitude0 = [50000]*dimension\n",
    "scale0 = [300]*dimension\n",
    "nu0 = 13/3\n",
    "model0 = ot.MaternModel(scale0, amplitude0, nu0)\n",
    "# Karhunen Loeve decomposition of process \n",
    "algorithm = ot.KarhunenLoeveP1Algorithm(mesh, model0, 1e-3)\n",
    "algorithm.run()\n",
    "resultsE = algorithm.getResult()\n",
    "resultsE.setName('E_')\n",
    "\n",
    "# Second Process D_ : Diameter\n",
    "amplitude = [.3]*dimension\n",
    "scale = [250]*dimension\n",
    "nu = 13/3\n",
    "model1 = ot.MaternModel(scale, amplitude, nu)\n",
    "algorithm = ot.KarhunenLoeveP1Algorithm(mesh, model1, 1e-3)\n",
    "algorithm.run()\n",
    "resultsD = algorithm.getResult()\n",
    "resultsD.setName('D_')\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# SECOND STEP : Definition of the random variables\n",
    "# random variable for the density of the material (kg/mÂ³)\n",
    "sigma       = 750\n",
    "nameD       = 'Rho'\n",
    "RV_Rho = ot.Normal(0, sigma)\n",
    "RV_Rho.setName(nameD)\n",
    "# random variable for the position of the force   (mm) \n",
    "sigma_f      = 50\n",
    "namePos     = 'FP'\n",
    "RV_Fpos = ot.Normal(0, sigma_f)\n",
    "RV_Fpos.setName(namePos)\n",
    "# random variable for the norm of the force    (N)\n",
    "sigma_Fnor    = 5.5\n",
    "nameNor       = 'FN'\n",
    "RV_Fnorm  = ot.Normal(0, sigma_Fnor)\n",
    "RV_Fnorm.setName(nameNor)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Then convert the distributions t processes over a mesh\n",
    "SP_Rho = variablesAsProcess(RV_Rho, mesh)\n",
    "SP_Fpos = variablesAsProcess(RV_Fpos, mesh)\n",
    "SP_Fnorm = variablesAsProcess(RV_Fnorm, mesh)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Then we can do the Karhunen Loeve decomposition of the distributions : \n",
    "algorithm0 = ot.KarhunenLoeveP1Algorithm(mesh, SP_Rho.getCovarianceModel(), 1e-3)\n",
    "algorithm0.run()\n",
    "resultsRho = algorithm0.getResult()\n",
    "resultsRho.setName('Rho_')\n",
    "\n",
    "algorithm1 = ot.KarhunenLoeveP1Algorithm(mesh, SP_Fpos.getCovarianceModel(), 1e-3)\n",
    "algorithm1.run()\n",
    "resultsFpos = algorithm1.getResult()\n",
    "resultsFpos.setName('Fpos_')\n",
    "\n",
    "algorithm2 = ot.KarhunenLoeveP1Algorithm(mesh, SP_Fnorm.getCovarianceModel(), 1e-3)\n",
    "algorithm2.run()\n",
    "resultsFnor = algorithm2.getResult()\n",
    "resultsFnor.setName('Fnorm_')\n",
    "\n",
    "# Finally we get a list of aggregated KarhunenLoeve results\n",
    "listOfKLRes = [resultsE, resultsD, resultsRho, resultsFpos, resultsFnor]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE :\n",
    "\n",
    "When defining a distribution as a process, as shown above, it is of utmost importance to modifiy his function so that it takes as an input a constant process, not a distribution anymore. This can also be easily avoided, if you take the first value of the field generated.\n",
    "\n",
    "###### Always multiple ways to proceed :\n",
    "\n",
    "In the example presented here, 2 ways of approaching the problem are possible. First we can create all our processes individually, decompose them individually with Karhunen Loeve, and pass a list of KarhunenLoeveResults to our new function wrapper. \n",
    "But this is only necessary if we would have meshes of different shapes, or if we would have defined the scalar distributions on smaller meshes. But as all processes are defined over the same mesh here, we could have aggregated the Processes, and directly decompose the AggregatedProcess. \n",
    "\n",
    "###### Note2 :\n",
    "\n",
    "When using the **KarhunenLoeveP1Algorithm** function, only the covariance function is decomposed this means **all data 'bout the means and the trend functions are FORGOTTEN**. You have to add these constants **Directly in yout function!** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First tests with new architecture. \n",
    "our base is a list of Karhunen Loeve Results. This will be passed into a contructor called AggregatedKarhunenLoeveResults. This  class is necessary as it is used to pass between the one dimensional space of the KarhunenLoeve Coefficients to the arbitrary dimension of the field linked to it. \n",
    "This class keeps all the data about the decomposition and allows to work on lists of processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AggregatedKLObj = klfs.AggregatedKarhunenLoeveResults(listOfKLRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class = AggregatedKarhunenLoeveResults| name = Unnamed| Aggregation Order = 5| Threshold = 0.001| Covariance Model 0 = class=MaternModel scale=class=Point name=Unnamed dimension=1 values=[300] amplitude=class=Point name=Unnamed dimension=1 values=[50000] nu=4.33333| Covariance Model 1 = class=MaternModel scale=class=Point name=Unnamed dimension=1 values=[250] amplitude=class=Point name=Unnamed dimension=1 values=[0.3] nu=4.33333| Covariance Model 2 = class=RankMCovarianceModel, variance=class=Point name=Unnamed dimension=1 values=[562500], covariance=class=CovarianceMatrix dimension=0 implementation=class=MatrixImplementation name=Unnamed rows=0 columns=0 values=[], basis=class=Basis coll=[class=Function name=Unnamed implementation=class=FunctionImplementation name=Unnamed description=[x,y0] evaluationImplementation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] gradientImplementation=class=SymbolicGradient name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] hessianImplementation=class=SymbolicHessian name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1]], functions=[class=Function name=Unnamed implementation=class=FunctionImplementation name=Unnamed description=[x,y0] evaluationImplementation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] gradientImplementation=class=SymbolicGradient name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] hessianImplementation=class=SymbolicHessian name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1]]| Covariance Model 3 = class=RankMCovarianceModel, variance=class=Point name=Unnamed dimension=1 values=[2500], covariance=class=CovarianceMatrix dimension=0 implementation=class=MatrixImplementation name=Unnamed rows=0 columns=0 values=[], basis=class=Basis coll=[class=Function name=Unnamed implementation=class=FunctionImplementation name=Unnamed description=[x,y0] evaluationImplementation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] gradientImplementation=class=SymbolicGradient name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] hessianImplementation=class=SymbolicHessian name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1]], functions=[class=Function name=Unnamed implementation=class=FunctionImplementation name=Unnamed description=[x,y0] evaluationImplementation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] gradientImplementation=class=SymbolicGradient name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] hessianImplementation=class=SymbolicHessian name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1]]| Covariance Model 4 = class=RankMCovarianceModel, variance=class=Point name=Unnamed dimension=1 values=[30.25], covariance=class=CovarianceMatrix dimension=0 implementation=class=MatrixImplementation name=Unnamed rows=0 columns=0 values=[], basis=class=Basis coll=[class=Function name=Unnamed implementation=class=FunctionImplementation name=Unnamed description=[x,y0] evaluationImplementation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] gradientImplementation=class=SymbolicGradient name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] hessianImplementation=class=SymbolicHessian name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1]], functions=[class=Function name=Unnamed implementation=class=FunctionImplementation name=Unnamed description=[x,y0] evaluationImplementation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] gradientImplementation=class=SymbolicGradient name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1] hessianImplementation=class=SymbolicHessian name=Unnamed evaluation=class=SymbolicEvaluation name=Unnamed inputVariablesNames=[x] outputVariablesNames=[y0] formulas=[1]]| Eigen Value 0 = class=Point name=Unnamed dimension=7 values=[1.41923e+12,7.0656e+11,2.58952e+11,8.06386e+10,2.37086e+10,7.02051e+09,2.16944e+09]| Eigen Value 1 = class=Point name=Unnamed dimension=8 values=[45.1513,26.2003,11.7135,4.46624,1.57881,0.547873,0.193305,0.0707329]| Eigen Value 2 = class=Point name=Unnamed dimension=1 values=[5.625e+08]| Eigen Value 3 = class=Point name=Unnamed dimension=1 values=[2.5e+06]| Eigen Value 4 = class=Point name=Unnamed dimension=1 values=[30250]| Mesh 0 = class=Mesh name=Unnamed dimension=1 vertices=class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=101 dimension=1 | Mesh 1 = class=Mesh name=Unnamed dimension=1 vertices=class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=101 dimension=1 | Mesh 2 = class=Mesh name=Unnamed dimension=1 vertices=class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=101 dimension=1 | Mesh 3 = class=Mesh name=Unnamed dimension=1 vertices=class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=101 dimension=1 | Mesh 4 = class=Mesh name=Unnamed dimension=1 vertices=class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=101 dimension=1 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AggregatedKLObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we initialize our toy example : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First: beam initialization with mesh\n",
    "PureBeam = pbe.PureBeam(mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we pass our model into our function wrapper. We have two functions : \n",
    "- One for single evaluations\n",
    "- One for batch evaluations\n",
    "##### - Only one of the functions is required. \n",
    "#### In the wrapper we also pas the AggregatedKarhunenLoeveResults object, as well as the number of outputs the function has. \n",
    "## WARNING : \n",
    "#### It is not the dimension of the output (eg. the dimension of a field) but it represents the numer of different outputs the function has, as it is assumed that the function returns a tuple of scalars, fields etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our function, we can pass it in our wrapper! \n",
    "FUNC = klfs.KarhunenLoeveGeneralizedFunctionWrapper(\n",
    "                                AggregatedKarhunenLoeveResults = AggregatedKLObj,\n",
    "                                func        = PureBeam.singleEval, \n",
    "                                func_sample = PureBeam.batchEval,\n",
    "                                n_outputs   = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the model is created as the AggregatedKarhunenLoeveResult object, we can begin our DOE (Design Of Experiment)\n",
    "- For this, we use our class called **KarhunenLoeveSobolIndicesExperiment**. \n",
    "- This last class aims to be similar to the class ot.SobolIndicesExperiment. \n",
    "##### Generation of the input design : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples A and B of size 2000 and dimension 18\n",
      "Experiment of size 14000 and dimension 18\n"
     ]
    }
   ],
   "source": [
    "N = 2000\n",
    "SobolExperiment = klfs.KarhunenLoeveSobolIndicesExperiment(AggregatedKLObj, N ,False)\n",
    "inputDesign = SobolExperiment.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation of the function on the input design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed:   34.5s\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done 677 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 714 tasks      | elapsed:   42.6s\n",
      "[Parallel(n_jobs=-1)]: Done 753 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   46.9s\n",
      "[Parallel(n_jobs=-1)]: Done 833 tasks      | elapsed:   49.2s\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:   51.5s\n",
      "[Parallel(n_jobs=-1)]: Done 917 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=-1)]: Done 960 tasks      | elapsed:   56.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:   59.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1050 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1097 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1193 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1293 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1344 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1397 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1505 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1617 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1674 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1733 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1853 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1914 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed:  1.9min finished\n",
      "/home/simady/anaconda/envs/pythontools/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape deflection:  (2000, 103)  should be [N,10X] something\n",
      "deflection std deviation  nan\n",
      "Using the batch evaluation function. Assumes that the outputs are in the \n",
      "same order than for the single evaluation function. This one should only \n",
      "return ProcessSamples, Samples, Lists or numpy arrays.\n",
      "Element is iterable, assumes that first dimension is size of sample\n",
      "Shape is (2000, 102) and dtype is <class 'numpy.float64'>\n",
      "Element 0 of the output tuple returns process samples of dimension 1\n",
      "Element is iterable, assumes that first dimension is size of sample\n",
      "Shape is (2000,) and dtype is <class 'numpy.float64'>\n",
      "Element 1 of the output tuple returns samples of dimension 1\n"
     ]
    }
   ],
   "source": [
    "outputDesign = FUNC(inputDesign[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to save samples as csv\n",
    "#import pandas as pd\n",
    "#inp = inputDesign[:2000]\n",
    "#out0 = outputDesign[0]\n",
    "#out1 = outputDesign[1]\n",
    "#inp.exportToCSVFile('./MetaModelSamples/sample2000_rand.csv')\n",
    "#vonMises = ot.Sample(np.array(np.stack([np.squeeze(np.asarray(out0[i])) for i in range(len(out0))])))\n",
    "#vonMises.exportToCSVFile('./MetaModelSamples/vonMises_rand.csv')\n",
    "#out1.exportToCSVFile('./MetaModelSamples/maxDefl_rand.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity  analysis part : \n",
    "##### We pass as an argument the estimator we want to use for the sensitivity analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldSensitivityAnalysis = klfs.SobolKarhunenLoeveFieldSensitivityAlgorithm()\n",
    "FieldSensitivityAnalysis.setDesign(inputDesign, outputDesign, N)\n",
    "FieldSensitivityAnalysis.setEstimator(ot.SaltelliSensitivityAlgorithm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldSensitivityAnalysis.getFirstOrderIndices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldSensitivityAnalysis.getFirstOrderIndicesInterval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldSensitivityAnalysis.getAggregatedFirstOrderIndices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldSensitivityAnalysis.getAggregatedTotalOrderIndices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldSensitivityAnalysis.getFirstOrderIndicesDistribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldSensitivityAnalysis.getTotalOrderIndices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldSensitivityAnalysis.getTotalOrderIndicesDistribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldSensitivityAnalysis.getTotalOrderIndicesInterval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldSensitivityAnalysis.getSecondOrderIndices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = FieldSensitivityAnalysis.__results__[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = x.draw(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
