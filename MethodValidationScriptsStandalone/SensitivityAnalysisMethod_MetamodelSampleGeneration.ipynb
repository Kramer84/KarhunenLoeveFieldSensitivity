{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity analysis on models governed by stochastic fields :\n",
    "## Analysis & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Date: 14.10.20\n",
    "- Author: K. A. Simady "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aim of the notebook : present the methods already developed as well as a simple example, and compare the already known methods to the ones developed. \n",
    "- **Theme :** Sensitivity analysis of models taking as an input scalar random variables, as well as random stochastic fields. \n",
    "- **Example :** Model of a beam being supported on both ends and subject to a point force. Five quantities are subject to uncertainties : \n",
    "    - The diameter D_ of the section of the beam is varying along a one dimensional field along the axis of the beam. \n",
    "    - The Young Modulus E_ is also varying stochastically along the same axis\n",
    "    - The position of the application point of the force FPos is following a scalar normal law centered on the beams middle point. \n",
    "    - The norm of the force vector FNorm is also follwing a scalar normal law centered around 100N \n",
    "    - The global densitiy of the beam which is also determined by a scalar normal law centered around the materials real density\n",
    "- **Method :** Roughly, the method consist in reexpressing the model (which was taking as inputs **Fields** & **Scalars**) as a new model only dependent of a **Scalar vector**. This is made possible through the usage of the **Karhunen-Loeve decomposition**, thanks to which one can freely express a field generated by a stochastic process of known parameters as a vector of scalars. This decomposition is analogous to the **Fourier** decomposition. Once the model is re-expressed a metamodel is built using either krieging or polynomial chaos. Then the sensitivity analysis is done on this metamodel, in hope that it behaves as the real model. This will be tested here.\n",
    "- **Application :** This method is developed to be used to analyse the model of a aeronautics grade heat exchanger, but this example will not be presented here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len vertices is: 101\n"
     ]
    }
   ],
   "source": [
    "# Let's first import the necessary modules. \n",
    "# Base modules :\n",
    "import os\n",
    "from collections.abc import Sequence, Iterable\n",
    "import numpy as np\n",
    "import openturns as ot\n",
    "import pandas as pd\n",
    "# Phimeca's modules :\n",
    "#import pythontools as pt # tools created for krieging, pce and other stuff... \n",
    "# Own modules : \n",
    "import KarhunenLoeveFieldSensitivity as klfs # integration of the codes developed ... \n",
    "# Example Model :\n",
    "import beamExample as MODEL # Could be any model really... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The inputs to our model are the 5 uncertain variables, which effects we try to measure. As for the Karhunen-Loeve decomposition the parameters of the stochastic process must be known (scale, amplitude, autocorrelation function) we will first define those. \n",
    "- Only two of our 5 inputs are stochastic fields, but we need to also express the other ones as fields. To achieve this, we can think about a scalar as the first value of a constant stochastic_field. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's define our processes :\n",
    "- As only the autocovariance function and the mesh is needed for the **Karhunen Loeve** decomposition, we have no need to neither define our gaussian processes nor to set a trend function. This has some implications :\n",
    "    - **The trend function was as a constant to represent the mean value of our process. As it is no longer present, in the decomposition, we have to directly add the means inside of our model and consider the inputs as sole perturbations of the mean.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first define the mesh, as it is the same for all the processes, random variables and the finite elements of the beam itself. \n",
    "dimension = 1\n",
    "NElem = [99]\n",
    "mesher = ot.IntervalMesher(NElem)\n",
    "lowerBound = [0] #mm\n",
    "upperBound = [1000] #mm\n",
    "interval = ot.Interval(lowerBound,upperBound)\n",
    "mesh = mesher.build(interval)\n",
    "# 100 elements of 10 mm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>MaternModel(scale=[300], amplitude=[10500], nu=4.33333)</p>"
      ],
      "text/plain": [
       "class=MaternModel scale=class=Point name=Unnamed dimension=1 values=[300] amplitude=class=Point name=Unnamed dimension=1 values=[10500] nu=4.33333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First process decomposition : Stochastic young's modulus \n",
    "amplitude0 = [210000*.05]*dimension # Mean : 210000 MPaa\n",
    "scale0 = [300]*dimension \n",
    "nu0 = 13/3\n",
    "model0 = ot.MaternModel(scale0, amplitude0, nu0)\n",
    "\n",
    "process=ot.GaussianProcess(model0, mesh)\n",
    "\n",
    "# Karhunen Loeve decomposition of process \n",
    "algorithm = ot.KarhunenLoeveP1Algorithm(mesh, model0, 1e-3)\n",
    "#algorithm.setNbModes(6)\n",
    "algorithm.run()\n",
    "resultsE = algorithm.getResult()\n",
    "resultsE.setName('E_')\n",
    "resultsE.getCovarianceModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second process decomposition : Stochastic Diameter\n",
    "amplitude = [10*.05]*dimension # Mean 10 mm\n",
    "scale = [250]*dimension\n",
    "nu = 7.4/3\n",
    "model1 = ot.MaternModel(scale, amplitude, nu)\n",
    "algorithm = ot.KarhunenLoeveP1Algorithm(mesh, model1, 1e-3)\n",
    "#algorithm.setNbModes(6)\n",
    "algorithm.run()\n",
    "resultsD = algorithm.getResult()\n",
    "resultsD.setName('D_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two scalar random variables : \n",
    "# random variable for the position of the force (mm) \n",
    "sigma_f      = 500*.05\n",
    "RV_Fpos = ot.Normal(500, sigma_f)\n",
    "RV_Fpos.setName('FPos')\n",
    "# random variable for the norm of the force (N)\n",
    "sigma_Fnor    = 100*.05\n",
    "RV_Fnorm  = ot.Normal(100, sigma_Fnor)\n",
    "RV_Fnorm.setName('FNorm')\n",
    "\n",
    "# With the modifs, we can now utilize directly the Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once all the processes are defined, created and decomposed thanks to Karhunen Loeve, we use our newly created class **AggregatedKarhunenLoeveResults**. This class is the link between our reduced normal centered law vector, being one dimensional, and our dimension of arbitrary fields and scalars (so either a-dimensional or multi-dimensional). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean value of distribution \"FPos\" at index 2 of type Normal is not 0.\n",
      "Distribution recentered and mean added to list of means\n",
      "Set the \"liftWithMean\" flag to true if you want to include the mean.\n",
      "The mean value of distribution \"FNorm\" at index 3 of type Normal is not 0.\n",
      "Distribution recentered and mean added to list of means\n",
      "Set the \"liftWithMean\" flag to true if you want to include the mean.\n"
     ]
    }
   ],
   "source": [
    "listOfKLRes = [resultsE, resultsD, RV_Fpos, RV_Fnorm]\n",
    "AggregatedKLRes = klfs.AggregatedKarhunenLoeveResults(listOfKLRes)\n",
    "AggregatedKLRes.setMean(0, 210000) # At other indices the means are initialized from the distributions\n",
    "AggregatedKLRes.setMean(1, 10) \n",
    "AggregatedKLRes.setLiftWithMean(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Depending on how our model was defined, some more steps may be performed if necessary. In our case, to initialize our model, we first have to pass it a mesh from which dimensions it builds its finite element representation, entirely based on the mesh on which the processes where defined. Then we differentiate our model into two functions, one that works on multiple inputs with multiprocessing and one only for single evaluations. **This differntation step is optional, only one of the two functions is needed**\n",
    "- **The functions as well as the aggregated Karhunen Loeve Results object are passed to a other newly defined class: KarhunenLoeveGeneralizedFunctionWrapper :\n",
    "    > As this class has access to the Karhunen-Loeve decomposition (which is as said the link between our differnt dimensions) and the model to analyse, this wrapper allows us to get a other view of the model, a view where the model is only dependent of a input vector of values following a centered reduced normal law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class = ComposedKarhunenLoeveResultsAndDistributions| name = Unnamed| Aggregation Order = 4| Threshold = 0.001| Covariance Model 0 = class=MaternModel scale=class=Point name=Unnamed dimension=1 values=[300] amplitude=class=Point name=Unnamed dimension=1 values=[10500] nu=4.33333| Covariance Model 1 = class=MaternModel scale=class=Point name=Unnamed dimension=1 values=[250] amplitude=class=Point name=Unnamed dimension=1 values=[0.5] nu=2.46667| Covariance Model 2 = None| Covariance Model 3 = None| Eigen Value 0 = class=Point name=Unnamed dimension=7 values=[6.25878e+10,3.11591e+10,1.14197e+10,3.55608e+09,1.04551e+09,3.09588e+08,9.56649e+07]| Eigen Value 1 = class=Point name=Unnamed dimension=10 values=[121.91,70.6145,32.8343,13.8746,5.79651,2.50434,1.14062,0.550531,0.281169,0.151298]| Eigen Value 2 = None| Eigen Value 3 = None| Mesh 0 = class=Mesh name=Unnamed dimension=1 vertices=class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=100 dimension=1 | Mesh 1 = class=Mesh name=Unnamed dimension=1 vertices=class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=100 dimension=1 | Mesh 2 = None| Mesh 3 = None"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definition of the model :\n",
    "_MODEL = MODEL.PureBeam()\n",
    "\n",
    "# initialization of the function wrapper : \n",
    "FUNC = klfs.KarhunenLoeveGeneralizedFunctionWrapper(\n",
    "                                AggregatedKarhunenLoeveResults = AggregatedKLRes,\n",
    "                                func        = None, \n",
    "                                func_sample = _MODEL.batchEval,\n",
    "                                n_outputs   = 2) #We have to define the number of elements in the tuple the model returns !!!!!!!!!!!\n",
    "AggregatedKLRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have defined our model and input parameters we can begin to design our DOE (Design of experiment). We will generate mutiple DOEs, as we want to compare multiple methods together. \n",
    "- **As we know that ALL the components of the vector entering into our wrapped functions are following a centered resuced normal law we have different ways of generating our DOE.**\n",
    "    - First of all we need 3 types of DOEs :\n",
    "        - One for the krieging with 100 points : This one will be generated with the LHS sampling method. For this generation we do not need any class in particular. \n",
    "            - => For this sample we also need the response (or output) of the finite element model (FUNC) \n",
    "        - Two for the calculus of the sobol indices. These ones will be much bigger and will only follow a random sampling method. Only one of these samples will be evaluated on the finite element model. The other one will be evaluated on the krieging metamodel to compare the sobol indices in both cases. \n",
    "    - As we want to test the inluence of the different parameters having an influence we will prepaper multiple DOEs per type of DOE. :\n",
    "        - For the first type (the krieging model):\n",
    "            - one DOE with 50 points following the LHS sampling method    | SEED : 130875 => CALCULUS OF THE FEM MODEL RESPONSE\n",
    "            - one DOE with 100 points following the LHS sampling method   | SEED : 409484 => CALCULUS OF THE FEM MODEL RESPONSE\n",
    "            - one DOE with 200 points following the LHS sampling method   | SEED : 224409 => CALCULUS OF THE FEM MODEL RESPONSE\n",
    "\n",
    "        - For the second and third type (sobol on the fem model):\n",
    "            - one DOE with 1000 points following a random sampling method | SEED : 248214 => CALUCLUS OF THE FEM MODEL RESPONSE\n",
    "            - one DOE with 2000 points following a random sampling method | SEED : 720442 => CALUCLUS OF THE FEM MODEL RESPONSE\n",
    "            - one DOE with 4000 points following a random sampling method | SEED : 109242 => CALUCLUS OF THE FEM MODEL RESPONSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "def timer(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_timer(*args, **kwargs):\n",
    "        tic = time.perf_counter()\n",
    "        value = func(*args, **kwargs)\n",
    "        toc = time.perf_counter()\n",
    "        elapsed_time = toc - tic\n",
    "        print(f\"Elapsed time: {elapsed_time:0.4f} seconds\")\n",
    "        return value\n",
    "    return wrapper_timer\n",
    "\n",
    "\n",
    "# Little helper class for optimized lhs :\n",
    "@timer\n",
    "def optimizedLHS(distribution, size, seed):\n",
    "    ot.RandomGenerator.SetSeed(seed)\n",
    "    lhs = ot.LHSExperiment(distribution, size, True, True)\n",
    "    lhs_optimise = ot.SimulatedAnnealingLHS(lhs)\n",
    "    lhs_sample = lhs_optimise.generate()\n",
    "    return lhs_sample\n",
    "\n",
    "@timer\n",
    "def getSample(distribution, size, seed):\n",
    "    ot.RandomGenerator.SetSeed(seed)\n",
    "    sample = distribution.getSample(size)\n",
    "    sample.setDescription(distribution.getDescription())\n",
    "    return sample\n",
    "\n",
    "@timer \n",
    "def getSobolExperiment(size, seed, secondOrder = False):\n",
    "    ot.RandomGenerator.SetSeed(seed)    \n",
    "    experiment = klfs.KarhunenLoeveSobolIndicesExperiment(AggregatedKLRes, size, secondOrder)\n",
    "    sobolExp = experiment.generate()\n",
    "    return sobolExp, experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our design of experiment, we will need some data about our KL decomposition, namely the order of decomposition and a random normal vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>ComposedDistribution(Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), Normal(mu = 0, sigma = 1), IndependentCopula(dimension = 19))</p>"
      ],
      "text/plain": [
       "class=ComposedDistribution name=ComposedDistribution dimension=19 copula=class=IndependentCopula name=IndependentCopula dimension=19 marginal[0]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[1]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[2]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[3]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[4]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[5]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[6]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[7]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[8]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[9]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[10]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[11]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[12]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[13]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[14]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[15]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[16]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[17]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1] marginal[18]=class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[0] sigma=class=Point name=Unnamed dimension=1 values=[1] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nModes = AggregatedKLRes.getSizeModes()  # the number of elements in the input vector of our KL wrapped model\n",
    "randNormVect = ot.ComposedDistribution([ot.Normal()] * nModes)  # \n",
    "randNormVect.setDescription(AggregatedKLRes._getModeDescription())\n",
    "randNormVect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples A and B of size 10 and dimension 19\n",
      "Experiment of size 60 and dimension 19\n",
      "Elapsed time: 0.0011 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10,  0],\n",
       "       [10,  1],\n",
       "       [10,  2],\n",
       "       [10,  3],\n",
       "       [10,  4],\n",
       "       [10,  5],\n",
       "       [10,  6],\n",
       "       [10,  7],\n",
       "       [10,  8],\n",
       "       [10,  9],\n",
       "       [10, 10],\n",
       "       [10, 11],\n",
       "       [10, 12],\n",
       "       [10, 13],\n",
       "       [10, 14],\n",
       "       [10, 15],\n",
       "       [10, 16],\n",
       "       [10, 17],\n",
       "       [10, 18],\n",
       "       [11,  0],\n",
       "       [11,  1],\n",
       "       [11,  2],\n",
       "       [11,  3],\n",
       "       [11,  4],\n",
       "       [11,  5],\n",
       "       [11,  6],\n",
       "       [11,  7],\n",
       "       [11,  8],\n",
       "       [11,  9],\n",
       "       [11, 10],\n",
       "       [11, 11],\n",
       "       [11, 12],\n",
       "       [11, 13],\n",
       "       [11, 14],\n",
       "       [11, 15],\n",
       "       [11, 16],\n",
       "       [11, 17],\n",
       "       [11, 18],\n",
       "       [12,  0],\n",
       "       [12,  1],\n",
       "       [12,  2],\n",
       "       [12,  3],\n",
       "       [12,  4],\n",
       "       [12,  5],\n",
       "       [12,  6],\n",
       "       [12,  7],\n",
       "       [12,  8],\n",
       "       [12,  9],\n",
       "       [12, 10],\n",
       "       [12, 11],\n",
       "       [12, 12],\n",
       "       [12, 13],\n",
       "       [12, 14],\n",
       "       [12, 15],\n",
       "       [12, 16],\n",
       "       [12, 17],\n",
       "       [12, 18],\n",
       "       [13,  0],\n",
       "       [13,  1],\n",
       "       [13,  2],\n",
       "       [13,  3],\n",
       "       [13,  4],\n",
       "       [13,  5],\n",
       "       [13,  6],\n",
       "       [13,  7],\n",
       "       [13,  8],\n",
       "       [13,  9],\n",
       "       [13, 10],\n",
       "       [13, 11],\n",
       "       [13, 12],\n",
       "       [13, 13],\n",
       "       [13, 14],\n",
       "       [13, 15],\n",
       "       [13, 16],\n",
       "       [13, 17],\n",
       "       [13, 18],\n",
       "       [14,  0],\n",
       "       [14,  1],\n",
       "       [14,  2],\n",
       "       [14,  3],\n",
       "       [14,  4],\n",
       "       [14,  5],\n",
       "       [14,  6],\n",
       "       [14,  7],\n",
       "       [14,  8],\n",
       "       [14,  9],\n",
       "       [14, 10],\n",
       "       [14, 11],\n",
       "       [14, 12],\n",
       "       [14, 13],\n",
       "       [14, 14],\n",
       "       [14, 15],\n",
       "       [14, 16],\n",
       "       [14, 17],\n",
       "       [14, 18],\n",
       "       [15,  0],\n",
       "       [15,  1],\n",
       "       [15,  2],\n",
       "       [15,  3],\n",
       "       [15,  4],\n",
       "       [15,  5],\n",
       "       [15,  6],\n",
       "       [15,  7],\n",
       "       [15,  8],\n",
       "       [15,  9],\n",
       "       [15, 10],\n",
       "       [15, 11],\n",
       "       [15, 12],\n",
       "       [15, 13],\n",
       "       [15, 14],\n",
       "       [15, 15],\n",
       "       [15, 16],\n",
       "       [15, 17],\n",
       "       [15, 18],\n",
       "       [16,  0],\n",
       "       [16,  1],\n",
       "       [16,  2],\n",
       "       [16,  3],\n",
       "       [16,  4],\n",
       "       [16,  5],\n",
       "       [16,  6],\n",
       "       [16,  7],\n",
       "       [16,  8],\n",
       "       [16,  9],\n",
       "       [16, 10],\n",
       "       [16, 11],\n",
       "       [16, 12],\n",
       "       [16, 13],\n",
       "       [16, 14],\n",
       "       [16, 15],\n",
       "       [16, 16],\n",
       "       [16, 17],\n",
       "       [16, 18],\n",
       "       [17,  0],\n",
       "       [17,  1],\n",
       "       [17,  2],\n",
       "       [17,  3],\n",
       "       [17,  4],\n",
       "       [17,  5],\n",
       "       [17,  6],\n",
       "       [17,  7],\n",
       "       [17,  8],\n",
       "       [17,  9],\n",
       "       [17, 10],\n",
       "       [17, 11],\n",
       "       [17, 12],\n",
       "       [17, 13],\n",
       "       [17, 14],\n",
       "       [17, 15],\n",
       "       [17, 16],\n",
       "       [17, 17],\n",
       "       [17, 18],\n",
       "       [18,  0],\n",
       "       [18,  1],\n",
       "       [18,  2],\n",
       "       [18,  3],\n",
       "       [18,  4],\n",
       "       [18,  5],\n",
       "       [18,  6],\n",
       "       [18,  7],\n",
       "       [18,  8],\n",
       "       [18,  9],\n",
       "       [18, 10],\n",
       "       [18, 11],\n",
       "       [18, 12],\n",
       "       [18, 13],\n",
       "       [18, 14],\n",
       "       [18, 15],\n",
       "       [18, 16],\n",
       "       [18, 17],\n",
       "       [18, 18],\n",
       "       [19,  0],\n",
       "       [19,  1],\n",
       "       [19,  2],\n",
       "       [19,  3],\n",
       "       [19,  4],\n",
       "       [19,  5],\n",
       "       [19,  6],\n",
       "       [19,  7],\n",
       "       [19,  8],\n",
       "       [19,  9],\n",
       "       [19, 10],\n",
       "       [19, 11],\n",
       "       [19, 12],\n",
       "       [19, 13],\n",
       "       [19, 14],\n",
       "       [19, 15],\n",
       "       [19, 16],\n",
       "       [19, 17],\n",
       "       [19, 18],\n",
       "       [20,  0],\n",
       "       [20,  1],\n",
       "       [20,  2],\n",
       "       [20,  3],\n",
       "       [20,  4],\n",
       "       [20,  5],\n",
       "       [20,  6],\n",
       "       [21,  0],\n",
       "       [21,  1],\n",
       "       [21,  2],\n",
       "       [21,  3],\n",
       "       [21,  4],\n",
       "       [21,  5],\n",
       "       [21,  6],\n",
       "       [22,  0],\n",
       "       [22,  1],\n",
       "       [22,  2],\n",
       "       [22,  3],\n",
       "       [22,  4],\n",
       "       [22,  5],\n",
       "       [22,  6],\n",
       "       [23,  0],\n",
       "       [23,  1],\n",
       "       [23,  2],\n",
       "       [23,  3],\n",
       "       [23,  4],\n",
       "       [23,  5],\n",
       "       [23,  6],\n",
       "       [24,  0],\n",
       "       [24,  1],\n",
       "       [24,  2],\n",
       "       [24,  3],\n",
       "       [24,  4],\n",
       "       [24,  5],\n",
       "       [24,  6],\n",
       "       [25,  0],\n",
       "       [25,  1],\n",
       "       [25,  2],\n",
       "       [25,  3],\n",
       "       [25,  4],\n",
       "       [25,  5],\n",
       "       [25,  6],\n",
       "       [26,  0],\n",
       "       [26,  1],\n",
       "       [26,  2],\n",
       "       [26,  3],\n",
       "       [26,  4],\n",
       "       [26,  5],\n",
       "       [26,  6],\n",
       "       [27,  0],\n",
       "       [27,  1],\n",
       "       [27,  2],\n",
       "       [27,  3],\n",
       "       [27,  4],\n",
       "       [27,  5],\n",
       "       [27,  6],\n",
       "       [28,  0],\n",
       "       [28,  1],\n",
       "       [28,  2],\n",
       "       [28,  3],\n",
       "       [28,  4],\n",
       "       [28,  5],\n",
       "       [28,  6],\n",
       "       [29,  0],\n",
       "       [29,  1],\n",
       "       [29,  2],\n",
       "       [29,  3],\n",
       "       [29,  4],\n",
       "       [29,  5],\n",
       "       [29,  6],\n",
       "       [30,  7],\n",
       "       [30,  8],\n",
       "       [30,  9],\n",
       "       [30, 10],\n",
       "       [30, 11],\n",
       "       [30, 12],\n",
       "       [30, 13],\n",
       "       [30, 14],\n",
       "       [30, 15],\n",
       "       [30, 16],\n",
       "       [31,  7],\n",
       "       [31,  8],\n",
       "       [31,  9],\n",
       "       [31, 10],\n",
       "       [31, 11],\n",
       "       [31, 12],\n",
       "       [31, 13],\n",
       "       [31, 14],\n",
       "       [31, 15],\n",
       "       [31, 16],\n",
       "       [32,  7],\n",
       "       [32,  8],\n",
       "       [32,  9],\n",
       "       [32, 10],\n",
       "       [32, 11],\n",
       "       [32, 12],\n",
       "       [32, 13],\n",
       "       [32, 14],\n",
       "       [32, 15],\n",
       "       [32, 16],\n",
       "       [33,  7],\n",
       "       [33,  8],\n",
       "       [33,  9],\n",
       "       [33, 10],\n",
       "       [33, 11],\n",
       "       [33, 12],\n",
       "       [33, 13],\n",
       "       [33, 14],\n",
       "       [33, 15],\n",
       "       [33, 16],\n",
       "       [34,  7],\n",
       "       [34,  8],\n",
       "       [34,  9],\n",
       "       [34, 10],\n",
       "       [34, 11],\n",
       "       [34, 12],\n",
       "       [34, 13],\n",
       "       [34, 14],\n",
       "       [34, 15],\n",
       "       [34, 16],\n",
       "       [35,  7],\n",
       "       [35,  8],\n",
       "       [35,  9],\n",
       "       [35, 10],\n",
       "       [35, 11],\n",
       "       [35, 12],\n",
       "       [35, 13],\n",
       "       [35, 14],\n",
       "       [35, 15],\n",
       "       [35, 16],\n",
       "       [36,  7],\n",
       "       [36,  8],\n",
       "       [36,  9],\n",
       "       [36, 10],\n",
       "       [36, 11],\n",
       "       [36, 12],\n",
       "       [36, 13],\n",
       "       [36, 14],\n",
       "       [36, 15],\n",
       "       [36, 16],\n",
       "       [37,  7],\n",
       "       [37,  8],\n",
       "       [37,  9],\n",
       "       [37, 10],\n",
       "       [37, 11],\n",
       "       [37, 12],\n",
       "       [37, 13],\n",
       "       [37, 14],\n",
       "       [37, 15],\n",
       "       [37, 16],\n",
       "       [38,  7],\n",
       "       [38,  8],\n",
       "       [38,  9],\n",
       "       [38, 10],\n",
       "       [38, 11],\n",
       "       [38, 12],\n",
       "       [38, 13],\n",
       "       [38, 14],\n",
       "       [38, 15],\n",
       "       [38, 16],\n",
       "       [39,  7],\n",
       "       [39,  8],\n",
       "       [39,  9],\n",
       "       [39, 10],\n",
       "       [39, 11],\n",
       "       [39, 12],\n",
       "       [39, 13],\n",
       "       [39, 14],\n",
       "       [39, 15],\n",
       "       [39, 16],\n",
       "       [40, 17],\n",
       "       [41, 17],\n",
       "       [42, 17],\n",
       "       [43, 17],\n",
       "       [44, 17],\n",
       "       [45, 17],\n",
       "       [46, 17],\n",
       "       [47, 17],\n",
       "       [48, 17],\n",
       "       [49, 17],\n",
       "       [50, 18],\n",
       "       [51, 18],\n",
       "       [52, 18],\n",
       "       [53, 18],\n",
       "       [54, 18],\n",
       "       [55, 18],\n",
       "       [56, 18],\n",
       "       [57, 18],\n",
       "       [58, 18],\n",
       "       [59, 18]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b=getSobolExperiment(10,1584)\n",
    "X = a \n",
    "X_A = X[:10,:]\n",
    "X_An = np.array(X_A)\n",
    "Y_Bn = np.array(X[10:20])\n",
    "X_n = np.array(X)\n",
    "Y = np.zeros(X_n.shape)\n",
    "for i in range(int(X_n.shape[0]/10)):\n",
    "    Y[i*10:(i+1)*10, :] = (X_n[i*10:(i+1)*10, :]==Y_Bn)\n",
    "u=ot.Sample(Y)\n",
    "np.argwhere(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.1165 seconds\n",
      "Elapsed time: 0.4605 seconds\n",
      "Elapsed time: 1.8271 seconds\n",
      "First DOE type done\n",
      "\n",
      "Samples A and B of size 1000 and dimension 19\n",
      "Experiment of size 6000 and dimension 19\n",
      "Elapsed time: 0.0063 seconds\n",
      "Samples A and B of size 2000 and dimension 19\n",
      "Experiment of size 12000 and dimension 19\n",
      "Elapsed time: 0.0124 seconds\n",
      "Samples A and B of size 4000 and dimension 19\n",
      "Experiment of size 24000 and dimension 19\n",
      "Elapsed time: 0.0250 seconds\n",
      "Samples A and B of size 8000 and dimension 19\n",
      "Experiment of size 48000 and dimension 19\n",
      "Elapsed time: 0.0515 seconds\n",
      "Generating samples for the second order indices\n",
      "Samples A and B of size 2000 and dimension 19\n",
      "Experiment for second order generated\n",
      "Experiment of size 20000 and dimension 19\n",
      "Elapsed time: 0.0177 seconds\n",
      "Sobol Samples OK.\n",
      "Elapsed time: 1.8121 seconds\n"
     ]
    }
   ],
   "source": [
    "kg_doe50_130875   = optimizedLHS(randNormVect, 50, 130875)\n",
    "kg_doe100_409484  = optimizedLHS(randNormVect, 100, 409484)\n",
    "kg_doe200_224409  = optimizedLHS(randNormVect, 200, 224409)\n",
    "print('First DOE type done\\n')\n",
    "\n",
    "si_doe1000_248214, expe_doe1000 = getSobolExperiment(1000, 248214)\n",
    "si_doe2000_720442, expe_doe2000 = getSobolExperiment(2000, 720442)\n",
    "si_doe4000_109242, expe_doe4000 = getSobolExperiment(4000, 109242)\n",
    "si_doe8000_439038, expe_doe8000 = getSobolExperiment(8000, 439038)\n",
    "\n",
    "si_sec_doe2000_8291001288, expe_doe2000_sec = getSobolExperiment(2000, 8291001288, True)\n",
    "\n",
    "print('Sobol Samples OK.')\n",
    "\n",
    "valid_doe200_502849 = optimizedLHS(randNormVect, 200, 502849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples A and B of size 10000 and dimension 19\n",
      "Experiment of size 60000 and dimension 19\n",
      "Elapsed time: 0.0690 seconds\n"
     ]
    }
   ],
   "source": [
    "si_doe4000_1092445, expe_doe4000_2 = getSobolExperiment(10000, 1092445)\n",
    "#si_doe50000_3846345, expe_doe50000_3846345 = getSobolExperiment(50000, 3846345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si_doe50000_3846345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifting as process sample\n",
      "field E shape (100, 100)\n",
      "var_Fnor shape (100,)\n",
      "field_E 210055.2129089407 field_D 10.002162804359951 var_Fpos 499.9458882511429 var_Fnor 100.02508378644619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of 100 | elapsed:    5.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.0s finished\n",
      "/home/motherloadubu/anaconda3/envs/field_analysis/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape deflection:  (100, 101)  should be [N,10X] something\n",
      "deflection std deviation  3.7359493612046033\n",
      "Using the batch evaluation function. Assumes that the outputs are in the\n",
      "same order than for the single evaluation function. This one should only\n",
      "return ProcessSamples, Samples, Lists or numpy arrays.\n",
      "Element is iterable, assumes that first dimension is size of sample\n",
      "Shape is (100, 100) and dtype is <class 'numpy.float64'>\n",
      "Element 0 of the output tuple returns process samples of dimension 1\n",
      "Element is iterable, assumes that first dimension is size of sample\n",
      "Shape is (1, 100) and dtype is <class 'numpy.float64'>\n",
      "Element 1 of the output tuple returns process samples of dimension 1\n"
     ]
    }
   ],
   "source": [
    "test = FUNC(kg_doe100_409484)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifting as process sample\n",
      "field E shape (60000, 100)\n",
      "var_Fnor shape (60000,)\n",
      "field_E 209863.40016176054 field_D 10.000706731657932 var_Fpos 500.3059303993097 var_Fnor 99.95005641395308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1740s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 208 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 242 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 352 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 394 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 436 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 482 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 528 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 578 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done 628 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 794 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 852 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 914 tasks      | elapsed:   22.7s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1042 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1108 tasks      | elapsed:   27.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1178 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1248 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1322 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1396 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1474 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:   39.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1634 tasks      | elapsed:   41.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1716 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1802 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1888 tasks      | elapsed:   47.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1978 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2068 tasks      | elapsed:   52.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2162 tasks      | elapsed:   54.4s\n",
      "[Parallel(n_jobs=-1)]: Done 2256 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2354 tasks      | elapsed:   59.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2452 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2554 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2656 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2762 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2868 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2978 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3088 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3202 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3316 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3434 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3552 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3674 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3796 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3922 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4048 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4178 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4308 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4442 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4576 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4714 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4852 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4994 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5136 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5282 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5428 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5578 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5728 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5882 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6036 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6194 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6352 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6514 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6676 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6842 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7008 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7178 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7348 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7522 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7696 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7874 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8052 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8234 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8416 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8602 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8788 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8978 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9168 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9362 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9556 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9754 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9952 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10154 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10356 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10562 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 10768 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 10978 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11188 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11402 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 11616 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 11834 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 12052 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 12274 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 12496 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 12722 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 12948 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 13178 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 13408 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 13642 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 13876 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 14114 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 14352 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 14594 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 14836 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 15082 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 15328 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 15578 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 15828 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 16082 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 16336 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16594 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 16852 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 17114 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 17376 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 17642 tasks      | elapsed:  7.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 17908 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 18178 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 18448 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 18722 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 18996 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 19274 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 19552 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done 19834 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 20116 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 20402 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 20688 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 20978 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 21268 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 21562 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 21856 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 22154 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 22452 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 22754 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 23056 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 23362 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 23668 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 23978 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 24288 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 24602 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 24916 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 25234 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 25552 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 25874 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 26196 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 26522 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 26848 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 27178 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 27508 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 27842 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=-1)]: Done 28176 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 28514 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 28852 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 29194 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done 29536 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 29882 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done 30228 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 30578 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 30928 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 31282 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 31636 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 31994 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 32352 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 32714 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-1)]: Done 33076 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 33442 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 33808 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done 34178 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 34548 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done 34922 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 35296 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done 35674 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 36052 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 36434 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 36816 tasks      | elapsed: 15.0min\n",
      "[Parallel(n_jobs=-1)]: Done 37202 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=-1)]: Done 37588 tasks      | elapsed: 15.3min\n",
      "[Parallel(n_jobs=-1)]: Done 37978 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 38368 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done 38762 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 39156 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=-1)]: Done 39554 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 39952 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=-1)]: Done 40354 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=-1)]: Done 40756 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done 41162 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=-1)]: Done 41568 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 41978 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 42388 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done 42802 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done 43216 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done 43634 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=-1)]: Done 44052 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done 44474 tasks      | elapsed: 18.2min\n",
      "[Parallel(n_jobs=-1)]: Done 44896 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=-1)]: Done 45322 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 45748 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done 46178 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=-1)]: Done 46608 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=-1)]: Done 47042 tasks      | elapsed: 19.6min\n",
      "[Parallel(n_jobs=-1)]: Done 47476 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 47914 tasks      | elapsed: 20.0min\n",
      "[Parallel(n_jobs=-1)]: Done 48352 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done 48794 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=-1)]: Done 49236 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 49682 tasks      | elapsed: 20.8min\n",
      "[Parallel(n_jobs=-1)]: Done 50128 tasks      | elapsed: 21.0min\n",
      "[Parallel(n_jobs=-1)]: Done 50578 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-1)]: Done 51028 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=-1)]: Done 51482 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done 51936 tasks      | elapsed: 21.8min\n",
      "[Parallel(n_jobs=-1)]: Done 52394 tasks      | elapsed: 22.0min\n",
      "[Parallel(n_jobs=-1)]: Done 52852 tasks      | elapsed: 22.2min\n",
      "[Parallel(n_jobs=-1)]: Done 53314 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=-1)]: Done 53776 tasks      | elapsed: 22.6min\n",
      "[Parallel(n_jobs=-1)]: Done 54242 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=-1)]: Done 54708 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=-1)]: Done 55178 tasks      | elapsed: 23.4min\n",
      "[Parallel(n_jobs=-1)]: Done 55648 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done 56122 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=-1)]: Done 56596 tasks      | elapsed: 24.1min\n",
      "[Parallel(n_jobs=-1)]: Done 57074 tasks      | elapsed: 24.4min\n",
      "[Parallel(n_jobs=-1)]: Done 57552 tasks      | elapsed: 24.7min\n",
      "[Parallel(n_jobs=-1)]: Done 58034 tasks      | elapsed: 24.9min\n",
      "[Parallel(n_jobs=-1)]: Done 58516 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=-1)]: Done 59002 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=-1)]: Done 59488 tasks      | elapsed: 25.7min\n",
      "[Parallel(n_jobs=-1)]: Done 60000 out of 60000 | elapsed: 26.0min finished\n",
      "/home/motherloadubu/anaconda3/envs/field_analysis/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape deflection:  (60000, 101)  should be [N,10X] something\n",
      "deflection std deviation  3.8319695302295598\n",
      "Using the batch evaluation function. Assumes that the outputs are in the\n",
      "same order than for the single evaluation function. This one should only\n",
      "return ProcessSamples, Samples, Lists or numpy arrays.\n",
      "Element is iterable, assumes that first dimension is size of sample\n",
      "Shape is (60000, 100) and dtype is <class 'numpy.float64'>\n",
      "Element 0 of the output tuple returns process samples of dimension 1\n",
      "Element is iterable, assumes that first dimension is size of sample\n",
      "Shape is (1, 60000) and dtype is <class 'numpy.float64'>\n",
      "Element 1 of the output tuple returns process samples of dimension 1\n"
     ]
    }
   ],
   "source": [
    "resp2 = FUNC(si_doe4000_1092445)\n",
    "# resp_doe50000_3846345 = FUNC(si_doe50000_3846345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vonMises = ot.Sample(np.array(np.stack([np.squeeze(np.asarray(resp2[0][i])) for i in range(len(resp2[0]))]))) \n",
    "maxDefl = resp2[1][0]\n",
    "si_doe4000_1092445.exportToCSVFile(os.path.join('./','inp_10000'+'.csv'), ';')\n",
    "vonMises.exportToCSVFile(os.path.join('./','out_10000'+'_VM'+'.csv'), ';')\n",
    "maxDefl.exportToCSVFile(os.path.join('./','out_10000'+'_MD'+'.csv'), ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vonMises_2 = ot.Sample(np.array(np.stack([np.squeeze(np.asarray(resp_doe50000_3846345[0][i])) for i in range(len(resp_doe50000_3846345[0]))]))) \n",
    "#maxDefl_2 = resp_doe50000_3846345[1][0]\n",
    "#si_doe4000_1092445.exportToCSVFile(os.path.join('./','inp_50000'+'.csv'), ';')\n",
    "#vonMises_2.exportToCSVFile(os.path.join('./','out_50000'+'_VM'+'.csv'), ';')\n",
    "#maxDefl_2.exportToCSVFile(os.path.join('./','out_50000'+'_MD'+'.csv'), ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have defined our design of expriments, we're going to evaluate the model on them, to get it's response. Then we're going to all save into csv files in  a folder, and use these as a basis for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOE_names      = ['kg_doe50_130875', 'kg_doe100_409484', 'kg_doe200_224409', 'si_doe1000_248214', 'si_doe2000_720442', \n",
    "                  'val_doe200_502849', 'si_sec_doe2000_8291001288' , 'si_doe4000_109242', 'si_doe8000_439038']\n",
    "DOE_resp_names = ['kg_doe50_130875_resp', 'kg_doe100_409484_resp', 'kg_doe200_224409_resp', 'si_doe1000_248214_resp', 'si_doe2000_720442_resp', \n",
    "                  'val_doe200_502849_resp', 'si_sec_doe2000_8291001288_resp', 'si_doe4000_109242_resp', 'si_doe8000_439038_resp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_DOEs = [kg_doe50_130875,kg_doe100_409484,kg_doe200_224409,si_doe1000_248214,si_doe2000_720442, valid_doe200_502849, si_sec_doe2000_8291001288, si_doe4000_109242, si_doe8000_439038] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './DOE_RESP2'\n",
    "for i, doe in enumerate(l_DOEs):\n",
    "    if not os.path.isfile(os.path.join(folder,DOE_names[i]+'.csv')):\n",
    "        resp = FUNC(doe)\n",
    "        vonMises = ot.Sample(np.array(np.stack([np.squeeze(np.asarray(resp[0][i])) for i in range(len(resp[0]))]))) \n",
    "        maxDefl = resp[1][0]\n",
    "        doe.exportToCSVFile(os.path.join(folder,DOE_names[i]+'.csv'), ';')\n",
    "        vonMises.exportToCSVFile(os.path.join(folder,DOE_resp_names[i]+'_VM'+'.csv'), ';')\n",
    "        maxDefl.exportToCSVFile(os.path.join(folder,DOE_resp_names[i]+'_MD'+'.csv'), ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
